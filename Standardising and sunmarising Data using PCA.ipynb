{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4caa18b3",
   "metadata": {},
   "source": [
    "# Standardising of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c45cdf",
   "metadata": {},
   "source": [
    "## what is data standardization in machine learning?\n",
    "\n",
    "--> Data standardization is the process of rescaling the attributes so that they have mean as 0 and variance as 1. The ultimate goal to perform standardization is to bring down all the features to a common scale without distorting the differences in the range of the values "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ab8fbe",
   "metadata": {},
   "source": [
    "## Important Library\n",
    "### Pandas\n",
    "### Numpy\n",
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1f71be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23f07838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Name', 'e_magic', 'e_cblp', 'e_cp', 'e_crlc', 'e_cparhdr',\n",
       "       'e_minalloc', 'e_maxalloc', 'e_ss', 'e_sp', 'e_csum', 'e_ip', 'e_cs',\n",
       "       'e_lfarlc', 'e_ovno', 'e_oemid', 'e_oeminfo', 'e_lfanew', 'Machine',\n",
       "       'NumberOfSections', 'TimeDateStamp', 'PointerToSymbolTable',\n",
       "       'NumberOfSymbols', 'SizeOfOptionalHeader', 'Characteristics', 'Magic',\n",
       "       'MajorLinkerVersion', 'MinorLinkerVersion', 'SizeOfCode',\n",
       "       'SizeOfInitializedData', 'SizeOfUninitializedData',\n",
       "       'AddressOfEntryPoint', 'BaseOfCode', 'ImageBase', 'SectionAlignment',\n",
       "       'FileAlignment', 'MajorOperatingSystemVersion',\n",
       "       'MinorOperatingSystemVersion', 'MajorImageVersion', 'MinorImageVersion',\n",
       "       'MajorSubsystemVersion', 'MinorSubsystemVersion', 'SizeOfHeaders',\n",
       "       'CheckSum', 'SizeOfImage', 'Subsystem', 'DllCharacteristics',\n",
       "       'SizeOfStackReserve', 'SizeOfStackCommit', 'SizeOfHeapReserve',\n",
       "       'SizeOfHeapCommit', 'LoaderFlags', 'NumberOfRvaAndSizes', 'Malware',\n",
       "       'SuspiciousImportFunctions', 'SuspiciousNameSection', 'SectionsLength',\n",
       "       'SectionMinEntropy', 'SectionMaxEntropy', 'SectionMinRawsize',\n",
       "       'SectionMaxRawsize', 'SectionMinVirtualsize', 'SectionMaxVirtualsize',\n",
       "       'SectionMaxPhysical', 'SectionMinPhysical', 'SectionMaxVirtual',\n",
       "       'SectionMinVirtual', 'SectionMaxPointerData', 'SectionMinPointerData',\n",
       "       'SectionMaxChar', 'SectionMainChar', 'DirectoryEntryImport',\n",
       "       'DirectoryEntryImportSize', 'DirectoryEntryExport',\n",
       "       'ImageDirectoryEntryExport', 'ImageDirectoryEntryImport',\n",
       "       'ImageDirectoryEntryResource', 'ImageDirectoryEntryException',\n",
       "       'ImageDirectoryEntrySecurity'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"dataset/file_pe_headers.csv\")\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d4d5394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Droping Name and MALWARE Column\n",
    "X=df.drop([\"Name\",\"Malware\"],axis=1).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa85d68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.         -0.03506542 -0.04751096 ... -0.07054894 -0.0198525\n",
      "  -0.04066791]\n",
      " [ 0.         -0.03506542 -0.04751096 ... -0.03849221 -0.02110877\n",
      "  -0.02469983]\n",
      " [ 0.         -0.03506542 -0.04751096 ... -0.07599254 -0.02110877\n",
      "  -0.04066791]\n",
      " ...\n",
      " [ 0.         -0.18093613 -0.04958686 ... -0.07296832 -0.02110877\n",
      "  -0.04066791]\n",
      " [ 0.         -0.03506542 -0.04751096 ... -0.06691988 -0.02110877\n",
      "  -0.04066791]\n",
      " [ 0.         -0.03506542 -0.04751096 ...  0.00021781 -0.02110877\n",
      "  -0.04066791]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "x_standard_scale=StandardScaler().fit_transform(X)\n",
    "print(x_standard_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4fc8d9",
   "metadata": {},
   "source": [
    "# Summarizing the Large data using PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acf33c8",
   "metadata": {},
   "source": [
    "## Introduction of PCA\n",
    "\n",
    "--> Principal Component Analysis (PCA) is a dimensionality reduction technique in machine learning that aims to reduce the complexity of a high-dimensional data set while retaining as much information as possible.\n",
    "\n",
    "--> It works by finding the directions (i.e., the \"principal components\") in the data that explain the maximum amount of variance. These principal components are orthogonal to each other and are ranked in terms of their importance in explaining the variance in the data.\n",
    "\n",
    "--> PCA can be used for various purposes, such as visualizing high-dimensional data in 2D or 3D, removing noise from the data, or improving the performance of machine learning algorithms by reducing the number of features in the data.\n",
    "\n",
    "--> In practice, PCA is implemented by first centering the data, then computing the covariance matrix, and finally performing singular value decomposition (SVD) on the covariance matrix to obtain the principal components. The number of components to retain can be determined using various methods, such as explained variance ratio or scree plot analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c96a7b8",
   "metadata": {},
   "source": [
    "## Advantages of using PCA on your dataset? Here are several reasons why you want to use PCA:\n",
    "\n",
    "Removes correlated features. PCA will help you remove all the features that are correlated, a phenomenon known as multi-collinearity. Finding features that are correlated is time consuming, especially if the number of features is large.\n",
    "Improves machine learning algorithm performance. With the number of features reduced with PCA, the time taken to train your model is now significantly reduced.\n",
    "Reduce overfitting. By removing the unnecessary features in your dataset, PCA helps to overcome overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bc72458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.13714096e-01 6.04526312e-02 5.35847638e-02 4.95286930e-02\n",
      " 4.08242868e-02 3.43687925e-02 3.32004002e-02 3.01112226e-02\n",
      " 2.86901095e-02 2.81624164e-02 2.54807940e-02 2.38845548e-02\n",
      " 2.22696648e-02 2.05755591e-02 1.82485433e-02 1.73648310e-02\n",
      " 1.66649078e-02 1.63647194e-02 1.52683994e-02 1.46357930e-02\n",
      " 1.45790542e-02 1.45535760e-02 1.44699413e-02 1.44154480e-02\n",
      " 1.42948516e-02 1.39221004e-02 1.35338124e-02 1.33766277e-02\n",
      " 1.32896667e-02 1.23472302e-02 1.20507834e-02 1.15452214e-02\n",
      " 1.13731313e-02 1.10939084e-02 1.07062189e-02 1.01649154e-02\n",
      " 9.90148375e-03 9.61478385e-03 9.17627698e-03 9.04802544e-03\n",
      " 8.66332999e-03 6.94752252e-03 6.84216033e-03 6.48244001e-03\n",
      " 5.95005317e-03 5.91335216e-03 5.41615029e-03 5.10640740e-03\n",
      " 4.83543074e-03 4.45888820e-03 4.29104432e-03 3.82076025e-03\n",
      " 3.79864324e-03 3.24146447e-03 3.18558571e-03 2.67004617e-03\n",
      " 2.03201471e-03 1.73591476e-03 1.65758475e-03 1.56708821e-03\n",
      " 1.38839592e-03 1.20694096e-03 8.20896559e-04 6.92520065e-04\n",
      " 2.79632267e-04 1.36614783e-04 6.56001071e-06 3.22441346e-07\n",
      " 1.26534195e-10 5.64125607e-34 5.64125607e-34 5.64125607e-34\n",
      " 5.64125607e-34 5.64125607e-34 5.64125607e-34 5.64125607e-34\n",
      " 5.63722303e-34]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca=PCA()\n",
    "pca.fit_transform(x_standard_scale)\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bca4400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000000000000002\n"
     ]
    }
   ],
   "source": [
    "print(sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0570d6ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
